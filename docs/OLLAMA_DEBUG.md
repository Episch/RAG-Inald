# üêõ Ollama Integration Debug Guide

## üîç Problem Analyse

**Status**: LlmConnector ist funktional ‚úÖ (Status 200, Version 0.11.7)  
**Problem**: `/api/generate` Endpunkt gibt **404 Not Found** zur√ºck  
**Ursache**: Wahrscheinlich **keine Modelle geladen** (`"models": []`)

## üö® Sofortma√ünahmen

### 1. **Modell installieren** (Hauptproblem)
```bash
# Pr√ºfe verf√ºgbare Modelle
ollama list

# Falls leer, installiere ein Modell
ollama pull llama3.2

# Oder ein kleineres Modell zum Testen
ollama pull tinyllama
```

### 2. **Ollama Status pr√ºfen**
```bash
# Pr√ºfe ob Ollama l√§uft
curl http://localhost:11434/api/version

# Pr√ºfe verf√ºgbare Modelle
curl http://localhost:11434/api/tags

# Test direkt gegen Ollama
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3.2", "prompt": "Hello", "stream": false}'
```

### 3. **Debug-Endpunkte nutzen**
```bash
# Vollst√§ndiges Ollama-Debug
curl http://localhost:8000/debug/ollama | jq

# Modell-Test
curl http://localhost:8000/test/llm/models | jq

# Synchroner Generation-Test
curl http://localhost:8000/test/llm/sync | jq
```

## üîß H√§ufige L√∂sungen

### **L√∂sung 1: Modell installieren**
```bash
ollama pull llama3.2
# Warte bis Download fertig ist (~4GB)

# Teste danach
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "async": false}'
```

### **L√∂sung 2: Ollama neustarten**
```bash
# Stoppe Ollama
pkill ollama

# Starte Ollama neu
ollama serve

# In neuem Terminal: Modell laden
ollama pull llama3.2
```

### **L√∂sung 3: Kleineres Modell testen**
```bash
# F√ºr schw√§chere Hardware
ollama pull tinyllama  # nur ~600MB

# Teste mit kleinem Modell
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hi", "model": "tinyllama", "async": false}'
```

### **L√∂sung 4: Ollama Update**
```bash
# Update Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Oder manuell
sudo systemctl restart ollama
```

## üìä Erwartete Responses

### ‚úÖ **Korrekter Status** (nach Modell-Installation)
```json
{
  "status": [
    {
      "service": "LlmConnector",
      "content": "0.11.7", 
      "status_code": 200,
      "healthy": true,
      "models": ["llama3.2", "tinyllama"] // üéØ NICHT LEER!
    }
  ]
}
```

### ‚úÖ **Erfolgreiche Generation**
```json
{
  "status": "completed",
  "request_id": "llm_66f5c2e...",
  "model": "llama3.2",
  "processing_time_seconds": 3.45,
  "response": "Hello! How can I help you today?",
  "metadata": {
    "total_duration": 3450000000,
    "eval_count": 12
  }
}
```

## üõ†Ô∏è Erweiterte Debug-Kommandos

### **Vollst√§ndiger Ollama-Check**
```bash
# 1. Service Status
systemctl status ollama

# 2. Installierte Modelle
ollama list

# 3. Ollama Version
ollama --version

# 4. Prozesse
ps aux | grep ollama

# 5. Port-Check
netstat -tlnp | grep 11434

# 6. Logs
journalctl -u ollama -f
```

### **Manual API Tests**
```bash
# Version Check (sollte funktionieren)
curl http://localhost:11434/api/version

# Models Check (sollte Modelle zeigen)
curl http://localhost:11434/api/tags | jq

# Generate Test (braucht geladenes Modell)
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "prompt": "Say hello",
    "stream": false
  }' | jq
```

## üéØ Wahrscheinlichste Ursachen

1. **90%**: Kein Modell installiert (`ollama list` ist leer)
2. **5%**: Ollama l√§uft nicht (`curl localhost:11434` fehlschl√§gt)
3. **3%**: Falsche Ollama-Version oder API-√Ñnderung
4. **2%**: Port/Netzwerk-Problem

## ‚úÖ Erfolgs-Indikatoren

Nach der Reparatur sollten funktionieren:
- ‚úÖ `/api/status` zeigt `"models": ["llama3.2"]` 
- ‚úÖ `/api/llm/generate` gibt 202 (async) oder 200 (sync)
- ‚úÖ `/test/llm/sync` zeigt erfolgreiche Generation
- ‚úÖ Message Worker verarbeitet LlmMessage ohne 404-Fehler

## üîó N√ºtzliche Links

- [Ollama Installation](https://ollama.ai/)
- [Ollama Models](https://ollama.ai/library)
- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)

---

**TL;DR**: F√ºhre `ollama pull llama3.2` aus, dann sollte alles funktionieren! üöÄ
