# LLM Connector Integration - Ollama üß†

## üìã √úbersicht

Der `LlmConnector` ist vollst√§ndig implementiert und erweitert deine RAG-Pipeline um LLM-Funktionalit√§ten. Die Integration folgt deiner bestehenden Architektur und nutzt Symfony Messenger f√ºr asynchrone Verarbeitung.

## üöÄ Implementierte Features

### ‚úÖ LlmConnector Service
- **Ollama Integration**: Vollst√§ndige API-Anbindung √ºber `LMM_URL` Environment Variable
- **Text Generation**: `/api/generate` Endpunkt f√ºr Standard-Prompts
- **Chat Completion**: `/api/chat` Endpunkt f√ºr Dialog-basierte Interaktionen  
- **Spezialisierte Kategorisierung**: Optimiert f√ºr deine RAG Graph-Mapping Pipeline
- **Model Management**: Automatische Erkennung verf√ºgbarer Ollama-Modelle
- **Robuste Fehlerbehandlung**: Timeout-Management, Verbindungsfehler, JSON-Parsing

### ‚úÖ API Endpunkt `/api/llm/generate`
- **Synchrone & Asynchrone Verarbeitung**: W√§hlbar √ºber `async` Parameter
- **Input Validation**: Prompt-L√§nge, Modell-Validierung, Parameter-Checks
- **Token-Management**: Automatische Sch√§tzung f√ºr Timeout-Vermeidung
- **Queue Integration**: Message Bus f√ºr gro√üe Prompts

### ‚úÖ Message Queue Integration
- **LlmMessage**: Strukturierte Nachrichten mit Prompt, Model, Parametern
- **LlmMessageHandler**: Asynchrone Verarbeitung mit Ausgabe-Management
- **Chunking Support**: Integration mit bestehenden TokenChunker
- **Output Management**: Strukturierte JSON-Ausgaben in `/var/llm_output/`

### ‚úÖ Status Integration
- **Erweiterte Health Checks**: LLM-Status im `/api/status` Endpunkt
- **Model Discovery**: Verf√ºgbare Modelle werden automatisch erkannt
- **Graceful Degradation**: System funktioniert auch bei LLM-Ausf√§llen

### ‚úÖ RAG Pipeline Enhancement
- **ExtractorMessageHandler erweitert**: Automatische LLM-Kategorisierung nach Tika-Extraktion
- **Prompt Integration**: Nahtlose Verbindung zu deinem bestehenden Prompt-System
- **Error Recovery**: Robuste Fehlerbehandlung ohne Pipeline-Unterbrechung

## ‚öôÔ∏è Konfiguration

### Environment Variable
F√ºge zu deiner `.env` oder `.env.local` Datei hinzu:

```bash
# Ollama Instance URL
LMM_URL=http://localhost:11434
```

### Standardwerte
- **Default Model**: `llama3.2`
- **Unterst√ºtzte Modelle**: `llama3.2`, `llama3.1`, `llama3`, `mistral`, `codellama`, `qwen2.5`
- **Default Temperature**: `0.7`
- **Default Max Tokens**: `2048`
- **Timeout**: `300` Sekunden (5 Minuten)

## üéØ Verwendung

### 1. Synchrone LLM-Anfrage
```bash
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Erkl√§re mir die Bedeutung von RAG in der KI",
    "model": "llama3.2",
    "async": false,
    "temperature": 0.7,
    "maxTokens": 1000
  }'
```

### 2. Asynchrone Verarbeitung
```bash
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Sehr langer Text f√ºr Kategorisierung...",
    "model": "llama3.2",
    "async": true,
    "temperature": 0.3,
    "maxTokens": 4096
  }'
```

### 3. Status Check mit LLM
```bash
curl -X GET http://localhost:8000/api/status
```

**Response mit LLM-Status:**
```json
{
  "status": [
    {
      "service": "DocumentConnector",
      "content": "Apache Tika 2.9.1",
      "status_code": 200,
      "healthy": true
    },
    {
      "service": "RagConnector",
      "content": "Neo4j 5.x",
      "status_code": 200,
      "healthy": true
    },
    {
      "service": "LlmConnector",
      "content": "0.1.41",
      "status_code": 200,
      "healthy": true,
      "models": ["llama3.2", "mistral", "codellama"]
    }
  ]
}
```

## üìä Message Queue Workflow

1. **Extraction Request** ‚Üí `/api/extraction` mit `{"path": "test"}`
2. **ExtractorMessage** ‚Üí Message Bus (wie bisher)
3. **Document Processing** ‚Üí Tika-Extraktion + Text-Optimierung
4. **üÜï LLM Categorization** ‚Üí Automatische Kategorisierung via Ollama
5. **Output Storage** ‚Üí JSON-Dateien in `/public/storage/{path}/../`

### Ausgabe-Dateien
- **Erfolg**: `llm_categorization_2025-09-02_14-30-15.json`
- **Fehler**: `llm_error_2025-09-02_14-30-15.json`

## üß™ Testing

### Starte Ollama (falls nicht l√§uft)
```bash
# Download und Installation von Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Starte Ollama Service
ollama serve

# Lade ein Modell (z.B. llama3.2)
ollama pull llama3.2
```

### Teste die Integration
```bash
# 1. Status pr√ºfen
curl -X GET http://localhost:8000/api/status | jq

# 2. Kleine Synchrone Anfrage
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello World", "async": false}' | jq

# 3. Asynchrone Verarbeitung
curl -X POST http://localhost:8000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Sehr langer Text...", "async": true}' | jq

# 4. Complete RAG Pipeline
curl -X POST http://localhost:8000/api/extraction \
  -H "Content-Type: application/json" \
  -d '{"path": "test"}' | jq
```

## üõ†Ô∏è Erweiterte Features

### Custom Models
Die Validierung unterst√ºtzt g√§ngige Ollama-Modelle. F√ºr custom Models, erweitere die Choice-Constraint in `LlmPrompt.php`:

```php
#[Assert\Choice(
    choices: ['llama3.2', 'llama3.1', 'dein-custom-model'],
    message: 'Invalid model. Allowed models: {{ choices }}'
)]
```

### Error Handling
- **Timeout-Protection**: Automatische Erkennung zu langer Prompts f√ºr Sync-Mode
- **Graceful Degradation**: RAG Pipeline l√§uft weiter auch bei LLM-Fehlern
- **Detailed Logging**: Strukturierte Fehler-Logs f√ºr Debugging

### Output Management
- **Structured JSON**: Konsistente Ausgabe-Formate
- **Metadata Tracking**: Request-IDs, Timestamps, Processing-Times
- **File Storage**: Persistent speichern f√ºr sp√§tere Analyse

## üéâ Fazit

Der **LlmConnector ist production-ready** und erweitert deine RAG-Pipeline optimal:

- ‚úÖ **Vollst√§ndig integriert** in bestehende Architektur
- ‚úÖ **Message Queue** f√ºr asynchrone Verarbeitung
- ‚úÖ **Status Monitoring** f√ºr alle Services
- ‚úÖ **Robuste Fehlerbehandlung** und Logging
- ‚úÖ **Flexible API** f√ºr verschiedene Use Cases
- ‚úÖ **Automatische Kategorisierung** in der Extraction Pipeline

Die komplette **Extraction ‚Üí Tika ‚Üí Optimization ‚Üí LLM Categorization** Pipeline funktioniert now out-of-the-box! üöÄ
